{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretching the cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# cuda settings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = slim.nets.vgg.vgg_16.default_image_size\n",
    "num_classes = 8\n",
    "num_channels = 3\n",
    "train_data_path = \"train.tfrecords\"\n",
    "valid_data_path = \"valid.tfrecords\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View all the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg = slim.nets.vgg\n",
    "# x = tf.placeholder(tf.float32,[None,224,224,3])\n",
    "# with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "#     _,_ = vgg.vgg_16(x,100)\n",
    "# slim.get_variables_to_restore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading then tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(data_path='train.tfrecords'):\n",
    "    feature = {'train/image':tf.FixedLenFeature([],tf.string), 'train/label':tf.FixedLenFeature([],tf.int64)}\n",
    "    filename_queue = tf.train.string_input_producer([data_path],num_epochs=10)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialzed_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(serialzed_example,features=feature)\n",
    "\n",
    "    image = tf.decode_raw(features[\"train/image\"],tf.float32)\n",
    "    label = tf.cast(features[\"train/label\"],tf.int32)\n",
    "\n",
    "    image = tf.reshape(image,[image_size,image_size,num_channels])\n",
    "    label = tf.one_hot(label,depth=num_classes)\n",
    "    images,labels = tf.train.shuffle_batch([image,label],batch_size=10,capacity=20,num_threads=1,min_after_dequeue=10)\n",
    "\n",
    "    return (images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validation_data(data_path='valid.tfrecords'):\n",
    "    feature = {'valid/image':tf.FixedLenFeature([],tf.string), 'valid/label':tf.FixedLenFeature([],tf.int64)}\n",
    "    filename_queue = tf.train.string_input_producer([data_path],num_epochs=1)\n",
    "\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialzed_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(serialzed_example,features=feature)\n",
    "\n",
    "    image = tf.decode_raw(features[\"valid/image\"],tf.float32)\n",
    "    label = tf.cast(features[\"valid/label\"],tf.int32)\n",
    "\n",
    "    image = tf.reshape(image,[image_size,image_size,num_channels])\n",
    "    label = tf.one_hot(label,depth=num_classes)\n",
    "    images,labels = tf.train.shuffle_batch([image,label],batch_size=100,capacity=100,num_threads=1,min_after_dequeue=10)\n",
    "\n",
    "    return (images,labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the default graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default graph.....\n",
      "\n",
      "Finalized the graph ... !\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "print(\"Loading default graph.....\")\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    model_path = \"./vgg_16.ckpt\"\n",
    "    assert(os.path.isfile(model_path))\n",
    "\n",
    "    # Defining the placeholder variables\n",
    "    x = tf.placeholder(tf.float32,shape=(None,image_size,image_size,3))\n",
    "    y = tf.placeholder(tf.int32,shape=(None,num_classes))\n",
    "    \n",
    "    # loading the vgg graph\n",
    "    vgg = slim.nets.vgg\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope(weight_decay=0.0001)):\n",
    "        logits,end_points = vgg.vgg_16(x,num_classes=num_classes,is_training=True)\n",
    "        \n",
    "\n",
    "\n",
    "    variables_to_restore = slim.get_variables_to_restore(exclude=['vgg_16/fc8'])\n",
    "    init_fn = slim.assign_from_checkpoint_fn(model_path,variables_to_restore)\n",
    "\n",
    "    fc8_variables = slim.get_variables('vgg_16/fc8')\n",
    "    fc8_init = tf.variables_initializer(fc8_variables)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y))\n",
    "\n",
    "    fc8_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    fc8_train_op = fc8_optimizer.minimize(loss,var_list=fc8_variables)\n",
    "\n",
    "    full_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "    full_train_op = full_optimizer.minimize(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "    # actual output\n",
    "    actual = tf.argmax(y,1)\n",
    "    # predicted output\n",
    "    prediction = tf.argmax(logits,1)\n",
    "        \n",
    "    # tf.get_default_graph().finalize()  \n",
    "    \n",
    "    print(\"\\nFinalized the graph ... !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vgg_16.ckpt\n",
      "Started Retraining ... !\n",
      "\n",
      "Loss after 0 epoch : 1.30\n",
      "Loss after 10 epoch : 2.23\n",
      "Loss after 20 epoch : 2.70\n",
      "Loss after 30 epoch : 3.87\n",
      "Loss after 40 epoch : 4.14\n",
      "Loss after 50 epoch : 3.38\n",
      "Loss after 60 epoch : 1.08\n",
      "Loss after 70 epoch : 1.18\n",
      "Validation Accuracy after retraining for 80 epoch : 0.10\n",
      "\n",
      " Started Fine Tuning .... !\n",
      "\n",
      "Loss after 0 epoch : 0.9571317434310913\n",
      "Loss after 100 epoch : 1.653702974319458\n",
      "Loss after 200 epoch : 0.8523305654525757\n",
      "Loss after 300 epoch : 0.31597936153411865\n",
      "Loss after 400 epoch : 0.11487337201833725\n",
      "Loss after 500 epoch : 0.3917526304721832\n",
      "Loss after 600 epoch : 0.3999579846858978\n",
      "Loss after 700 epoch : 0.005485336296260357\n",
      "Validation accuracy after fune tuning for epoch 720 : 0.8199999928474426\n"
     ]
    }
   ],
   "source": [
    "# checking if the convnet graph is same as the default graph\n",
    "assert logits.graph == graph\n",
    "\n",
    "# start session and train the model\n",
    "with tf.Session(graph=graph,config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    images,labels = read_training_data(train_data_path)\n",
    "    valid_images,valid_labels = read_validation_data(valid_data_path)\n",
    "    \n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())  \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    init_fn(sess)\n",
    "    sess.run(fc8_init)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    print(\"\\n  Started Retraining ... !\\n\")\n",
    "\n",
    "    # Retraining last layer\n",
    "    for epoch in range(80):\n",
    "        img,lbl = sess.run([images,labels])\n",
    "        img = img.astype(np.uint8)\n",
    "        sess.run(fc8_train_op,feed_dict={x:img,y:lbl})\n",
    "        if(epoch%10 == 0):\n",
    "            print(\"Loss after {} epoch : {:.2f}\".format(epoch,sess.run(loss,feed_dict={x:img,y:lbl})))\n",
    "        \n",
    "    val_img,val_lbl = sess.run([valid_images,valid_labels])\n",
    "    val_img = val_img.astype(np.uint8)\n",
    "    print(\"Validation Accuracy after retraining for {} epoch : {:.2f}\".format(epoch+1,sess.run(accuracy,feed_dict={x:val_img,y:val_lbl})))\n",
    "\n",
    "\n",
    "    print(\"\\n  Started Fine Tuning .... !\\n\")\n",
    "    # Fine tuning all layers\n",
    "    epoch=0\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            img,lbl = sess.run([images,labels])\n",
    "            sess.run(full_train_op,feed_dict={x:img,y:lbl})\n",
    "            if(epoch%100 == 0):\n",
    "                print(\"Loss after {} epoch : {}\".format(epoch,sess.run(loss,feed_dict={x:img,y:lbl})))\n",
    "            epoch+=1\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        coord.request_stop(e)\n",
    "        \n",
    "    val_img,val_lbl = sess.run([valid_images,valid_labels])\n",
    "    print(\"Validation accuracy after fune tuning for {} epoch : {}\".format(epoch,sess.run(accuracy,feed_dict={x:val_img,y:val_lbl})))  \n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
